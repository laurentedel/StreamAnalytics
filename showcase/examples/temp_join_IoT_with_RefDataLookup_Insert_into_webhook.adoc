= Trigger webhook based on SQL result

Webhooks are _"user-defined HTTP callbacks"_. They are usually triggered by some event, such as pushing code to a repository[3] or a comment being posted to a blog.[4] When that event occurs, the source site makes an HTTP request to the URL configured for the webhook. Users can configure them to cause events on one site to invoke behavior on another.

same as before: Both tables `IoT` (insert) and `RefData_GeoLocation` (upsert) are representing a Kafka topic.

== Use case overview:
image::../../images/TemporalJoinIoTSensorWithLatestGeoLocInsertIntoWebHook.png[width=800]

== download generator:
[source,bash]
----
wget https://github.com/zBrainiac/StreamAnalytics/releases/download/StreamAnalytics_0.0.3/StreamAnalytics-0.0.3.0.jar
----

=== starting producers: `RefData_GeoLocation`

[source,shell script]
----
java -classpath StreamAnalytics-0.0.3.0.jar producer.RefDataGeoLocation localhost:9092
----

=== Console output
After the key is defined as an integer, an additional parameter must be given to the kafka-console-consumer:

_--key-deserializer "org.apache.kafka.common.serialization.IntegerDeserializer"_

[source,shell script]
----
./bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic refdata_geoLocation --property print.key=true --property key.separator=" - " --key-deserializer "org.apache.kafka.common.serialization.IntegerDeserializer"
0 - {"loc_id":0,"city":"Porrentruy","lon":47.415327,"lat":7.075221}
1 - {"loc_id":1,"city":"Geneva","lon":46.195602,"lat":6.148113}
2 - {"loc_id":2,"city":"Zürich","lon":47.366667,"lat":8.55}
3 - {"loc_id":3,"city":"Basel","lon":47.558395,"lat":7.573271}
4 - {"loc_id":4,"city":"Bern","lon":46.916667,"lat":7.466667}
5 - {"loc_id":5,"city":"Lausanne","lon":46.533333,"lat":6.666667}
6 - {"loc_id":6,"city":"Lucerne","lon":47.083333,"lat":8.266667}
...
----

=== starting producers: `iot`

----
java -classpath StreamAnalytics-0.0.3.0.jar producer.IoTSensorSimulatorAnomaly localhost:9092
----

=== Console output

[source,shell script]
----
./bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic iot --property print.key=true --property key.separator=" - "

1636190810020:26 - {"sensor_ts":1636190810020,"sensor_id":26,"sensor_0":5,"sensor_1":9,"sensor_2":7,"sensor_3":29,"sensor_4":2,"sensor_5":39,"sensor_6":4,"sensor_7":46,"sensor_8":84,"sensor_9":45,"sensor_10":640,"sensor_11":1090}
1636190810225:30 - {"sensor_ts":1636190810225,"sensor_id":30,"sensor_0":5,"sensor_1":6,"sensor_2":18,"sensor_3":24,"sensor_4":39,"sensor_5":47,"sensor_6":35,"sensor_7":37,"sensor_8":42,"sensor_9":40,"sensor_10":842,"sensor_11":482}
1636190810432:6 - {"sensor_ts":1636190810432,"sensor_id":6,"sensor_0":2,"sensor_1":6,"sensor_2":14,"sensor_3":24,"sensor_4":41,"sensor_5":36,"sensor_6":42,"sensor_7":0,"sensor_8":27,"sensor_9":82,"sensor_10":254,"sensor_11":179}
1636190810637:37 - {"sensor_ts":1636190810637,"sensor_id":37,"sensor_0":1,"sensor_1":4,"sensor_2":1,"sensor_3":5,"sensor_4":2,"sensor_5":44,"sensor_6":40,"sensor_7":26,"sensor_8":42,"sensor_9":94,"sensor_10":357,"sensor_11":477}
1636190810842:36 - {"sensor_ts":1636190810842,"sensor_id":36,"sensor_0":3,"sensor_1":0,"sensor_2":10,"sensor_3":30,"sensor_4":23,"sensor_5":5,"sensor_6":54,"sensor_7":9,"sensor_8":21,"sensor_9":2,"sensor_10":569,"sensor_11":1016}
1636190811052:35 - {"sensor_ts":1636190811052,"sensor_id":35,"sensor_0":6,"sensor_1":9,"sensor_2":7,"sensor_3":14,"sensor_4":33,"sensor_5":23,"sensor_6":5,"sensor_7":58,"sensor_8":87,"sensor_9":50,"sensor_10":365,"sensor_11":742}

...
----


==  Analytics Event Streaming

Once everything is up and running, you can reach the SQL Stream Builder Console at: http://localhost:8000[localhost:8000] +
The default login and password are “admin" / "admin”.

Quick intro in how to use the Streaming SQL Console: https://docs.cloudera.com/csa/1.5.1/ssb-sql-console/topics/csa-ssb-using-console.html[official SSB DOC]

=== Create a table

[source,sql]
----
SHOW Tables;
DROP TABLE `RefData_GeoLocation`;

CREATE TABLE `RefData_GeoLocation` (
  `loc_id` INT,
  `city` STRING,
  `lon` DOUBLE,
  `lat` DOUBLE,
  `eventTimestamp` TIMESTAMP(3) METADATA FROM 'timestamp',
  WATERMARK FOR `eventTimestamp` AS `eventTimestamp` - INTERVAL '15' SECONDS,
  PRIMARY KEY (loc_id) NOT ENFORCED
) COMMENT 'RefData_GeoLocation'
WITH (
  'connector' = 'upsert-kafka',
  'topic' = 'refdata_geoLocation',
  'properties.bootstrap.servers' = 'kafka:9092',
  'properties.group.id' = 'RefData_GeoLocation',
  'key.format' = 'raw',
  'value.format' = 'json'
);


DROP TABLE `IoT_Raw`;

CREATE TABLE `IoT_Raw` (
  `sensor_ts` BIGINT,
  `sensor_id` INT,
  `sensor_0` BIGINT,
  `sensor_1` BIGINT,
  `sensor_2` BIGINT,
  `sensor_3` BIGINT,
  `sensor_4` BIGINT,
  `sensor_5` BIGINT,
  `sensor_6` BIGINT,
  `sensor_7` BIGINT,
  `sensor_8` BIGINT,
  `sensor_9` BIGINT,
  `sensor_10` BIGINT,
  `sensor_11` BIGINT,
  `eventTimestamp` TIMESTAMP(3) METADATA FROM 'timestamp',
  WATERMARK FOR `eventTimestamp` AS `eventTimestamp` - INTERVAL '3' SECOND
) COMMENT 'iot_enriched_source'
WITH (
  'connector' = 'kafka',
  'topic' = 'iot',
  'properties.bootstrap.servers' = 'kafka:9092',
  'properties.auto.offset.reset' = 'earliest',
  'format' = 'json',
  'scan.startup.mode' = 'earliest-offset',
  'properties.group.id' = 'iot'
);
----

== Perform an HTTP action (webhook)
First you have to configure the *webhook table* to perform an HTTP action per message (sql result).

WARNING: Open new browser in _incognito mode_ and create a new http endpoint at https://webhook.site

. Select Console from the main menu.
. Select the *Tables* tab.
. Select *Add table > Webhook*. The *Webhook Table* window appears.
+

[source,shell]
----
Table name              webhook_table
Http EndPoint           webhook <Your unique URL endpoint https:// >
HttpMethod              POST
Disable SSL Validation  no
Enable Request Template yes
----
+
. In the *Code* tab, you can specify a code block that controls how the webhook displays the data.
For a webhook that is called for each message the following code is used:
+
image::../../images/ssb-webhooktable-code.png[width=600]
+
[source,javascript]
----
// Boolean function that takes entire row from query as Json Object
function onCondition(rowAsJson)
{return true;   // return false here for no-op, or plug in custom
        logic}
onCondition($p0)
----
+
. On the *Http Headers* tab, add HTTP headers using the HTTP Headers tab, if needed.
+
image::../../images/ssb-webhooktable-httpheader.png[width=600]
+
[source,shell]
----
Http Header     Content-Type
Value           application/json
----
+
. Press the *Add Header* sign to save
. On the *Request Template* tab modify template
+
image::../../images/ssb-webhooktable-requesttemplate.png[width=600]
+
[source,javascript]
----
{
   "incident":{
      "type":"incident",
      "title":"${sensor_id} - pressure is too high!",
        "body":{
   "type":"incident_body",
         "details":"Sensor with id ${sensor_id} has ${sensor_0} pascal, City:${city}, GeoLoc:${lon},${lat}"
      }
    }
}
----
. Click *Save changes*

=== Create a query
On the SSB UI:

. Click on Console (on the left bar) *> Compose > SQL*
+
. Enter `webhook_IoTLoc` for the *SQL Job Name* field.
+
. In the SQL box type the query shown below.
+
[source,sql]
----
INSERT INTO webhook_table
SELECT
  i.`sensor_ts`,
  i.`sensor_id`,
  i.`sensor_0`,
  geo.`city`,
  geo.`lon`,
  geo.`lat`
FROM `IoT_Raw` i
JOIN `RefData_GeoLocation` FOR SYSTEM_TIME AS OF i.`eventTimestamp` AS geo
ON i.`sensor_id` = geo.`loc_id`;
----
+
. Click *Execute* query
. Back on the https://webhook.site page you will see the submitted messages

+
[WARNING]
====
Make sure to stop your queries to release all resources once you finish. CSA CE is limited to a few worker tasks. You can double-check that all queries/jobs have been stopped by clicking on the SQL Jobs tab. If any jobs are still running, you can stop them from that page.
====