= Dynamic Data Quality Gate

== download generator:
[source,bash]
----
wget https://github.com/zBrainiac/StreamAnalytics/releases/download/StreamAnalytics_0.0.3/StreamAnalytics-0.0.3.0.jar
----

=== starting producers: `iot`

----
java -classpath StreamAnalytics-0.0.3.0.jar producer.IoTSensorSimulatorAnomaly localhost:9092
----

=== Console output

[source,shell script]
----
./bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic iot --property print.key=true --property key.separator=" - "

1636190810020:26 - {"sensor_ts":1636190810020,"sensor_id":26,"sensor_0":5,"sensor_1":9,"sensor_2":7,"sensor_3":29,"sensor_4":2,"sensor_5":39,"sensor_6":4,"sensor_7":46,"sensor_8":84,"sensor_9":45,"sensor_10":640,"sensor_11":1090}
1636190810225:30 - {"sensor_ts":1636190810225,"sensor_id":30,"sensor_0":5,"sensor_1":6,"sensor_2":18,"sensor_3":24,"sensor_4":39,"sensor_5":47,"sensor_6":35,"sensor_7":37,"sensor_8":42,"sensor_9":40,"sensor_10":842,"sensor_11":482}
1636190810432:6 - {"sensor_ts":1636190810432,"sensor_id":6,"sensor_0":2,"sensor_1":6,"sensor_2":14,"sensor_3":24,"sensor_4":41,"sensor_5":36,"sensor_6":42,"sensor_7":0,"sensor_8":27,"sensor_9":82,"sensor_10":254,"sensor_11":179}
1636190810637:37 - {"sensor_ts":1636190810637,"sensor_id":37,"sensor_0":1,"sensor_1":4,"sensor_2":1,"sensor_3":5,"sensor_4":2,"sensor_5":44,"sensor_6":40,"sensor_7":26,"sensor_8":42,"sensor_9":94,"sensor_10":357,"sensor_11":477}
1636190810842:36 - {"sensor_ts":1636190810842,"sensor_id":36,"sensor_0":3,"sensor_1":0,"sensor_2":10,"sensor_3":30,"sensor_4":23,"sensor_5":5,"sensor_6":54,"sensor_7":9,"sensor_8":21,"sensor_9":2,"sensor_10":569,"sensor_11":1016}
1636190811052:35 - {"sensor_ts":1636190811052,"sensor_id":35,"sensor_0":6,"sensor_1":9,"sensor_2":7,"sensor_3":14,"sensor_4":33,"sensor_5":23,"sensor_6":5,"sensor_7":58,"sensor_8":87,"sensor_9":50,"sensor_10":365,"sensor_11":742}

...
----

== QA reference

=== Load test QA reference test values
For a first test we can use the Kafka CLI utility to load some example QA reference test values. In a fully automated setup this values can be set by an external data profiler like deequ.

Kafka CLI utility:

[source,shell script]
----
./bin/kafka-console-producer.sh --bootstrap-server kafka:9092 --topic refdata_qa --property "parse.key=true" --property "key.separator= - "
----
example qa ref. values
[source,shell script]
----
default.IoT_Raw.sensor_0 - {"table_field":"default.IoT_Raw_sensor_0","lastValue":7,"minValue":0,"maxValue":10}
default.IoT_Raw.sensor_1 - {"table_field":"default.IoT_Raw_sensor_1","lastValue":7,"minValue":0,"maxValue":10}
default.IoT_Raw.sensor_2 - {"table_field":"default.IoT_Raw_sensor_2","lastValue":7,"minValue":0,"maxValue":10}
----

For the verification we can use again the Kafka CLI utilities:
[source,shell script]
----
./bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic refdata_qa --from-beginning  --property print.key=true --property key.separator=" - "
----
example result qa ref. values
[source,shell script]
----
default.IoT_Raw.sensor_0 - {"table_field":"default.IoT_Raw_sensor_0","lastValue":7,"minValue":0,"maxValue":10}
default.IoT_Raw.sensor_1 - {"table_field":"default.IoT_Raw_sensor_1","lastValue":7,"minValue":0,"maxValue":10}
default.IoT_Raw.sensor_2 - {"table_field":"default.IoT_Raw_sensor_2","lastValue":7,"minValue":0,"maxValue":10}
----


==  Analytics Event Streaming

Once everything is up and running, you can reach the SQL Stream Builder Console at: http://localhost:8000[localhost:8000] +
The default login and password are “admin" / "admin”.

Quick intro in how to use the Streaming SQL Console: https://docs.cloudera.com/csa/1.5.1/ssb-sql-console/topics/csa-ssb-using-console.html[official SSB DOC]

=== Create a table for QA reference values per column (upsert)

We just wanna keep the latest version of the qa ref. values therefore we use the upsert-kafka connector with the `table_field` e.g. default.IoT_Raw_sensor_0 as a primary key.

[source,sql]
----
SHOW TABLES;
DROP TEMPORARY TABLE `refdata_qa`
CREATE TEMPORARY TABLE `refdata_qa` (
  `table_field` STRING,
  `lastValue` DOUBLE,
  `minValue` DOUBLE,
  `maxValue` DOUBLE,
  `eventTimestamp` TIMESTAMP(3) METADATA FROM 'timestamp',
  WATERMARK FOR `eventTimestamp` AS `eventTimestamp` - INTERVAL '3' SECOND,
  PRIMARY KEY (table_field) NOT ENFORCED
) COMMENT 'refdata_qa'
WITH (
  'connector' = 'upsert-kafka',
  'properties.bootstrap.servers' = 'kafka:9092',
  'properties.auto.offset.reset' = 'earliest',
  'properties.request.timeout.ms' = '120000',
  'properties.transaction.timeout.ms' = '900000',
  'key.format' = 'raw',
  'value.format' = 'json',
  'topic' = 'refdata_qa'
);

SELECT * FROM `refdata_qa` ;
----
=== Create a table for IoT events (insert)
[source,sql]
----
CREATE TEMPORARY TABLE `IoT_Raw` (
  `sensor_ts` BIGINT,
  `sensor_id` BIGINT,
  `sensor_0` BIGINT,
  `sensor_1` BIGINT,
  `sensor_2` BIGINT,
  `sensor_3` BIGINT,
  `sensor_4` BIGINT,
  `sensor_5` BIGINT,
  `sensor_6` BIGINT,
  `sensor_7` BIGINT,
  `sensor_8` BIGINT,
  `sensor_9` BIGINT,
  `sensor_10` BIGINT,
  `sensor_11` BIGINT,
  `eventTimestamp` TIMESTAMP(3) METADATA FROM 'timestamp',
  WATERMARK FOR `eventTimestamp` AS `eventTimestamp` - INTERVAL '3' SECOND
) COMMENT 'IoT_Raw'
WITH (
  'properties.bootstrap.servers' = 'kafka:9092',
  'properties.auto.offset.reset' = 'earliest',
  'connector' = 'kafka',
  'format' = 'json',
  'topic' = 'iot',
  'properties.group.id' = 'IoT_Raw',
  'scan.startup.mode' = 'earliest-offset'
);
----

=== Data Quality Gate

[source,sql]
----
SELECT
  i.`sensor_ts`,
  i.`sensor_id`,
  i.`sensor_0`,
  qa.`lastValue` AS `sensor_0_lastValue`,
  qa.`minValue`AS `sensor_0_minValue`,
  qa.`maxValue` AS `sensor_0_maxValue`
FROM `IoT_Raw` i, `refdata_qa` qa
WHERE qa.`table_field` = 'default.IoT_Raw_sensor_0'
  AND i.`sensor_0` NOT BETWEEN  qa.`minValue` AND qa.`maxValue`; -- value validation against QA values
----