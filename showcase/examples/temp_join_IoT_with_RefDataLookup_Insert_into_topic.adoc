= Temporal join of IoT events with the geolocation reference data

same as before: Both tables `IoT` (insert) and `RefData_GeoLocation` (upsert) are representing a Kafka topic.

But this time we publish the results of the continuously running SQL into new Kafka topic (sink).

== Use case overview:
image::../../images/TemporalJoinIoTSensorWithLatestGeoLocInsertIntoKafka.png[width=800]

== download generator:
[source,bash]
----
wget https://github.com/zBrainiac/StreamAnalytics/releases/download/StreamAnalytics_0.1.0/StreamAnalytics-0.2.0.0.jar
----

=== starting producers: `RefData_GeoLocation`

[source,shell script]
----
java -classpath StreamAnalytics-0.2.0.0.jar producer.RefDataGeoLocation localhost:9092
----

=== Console output
After the key is defined as an integer, an additional parameter must be given to the kafka-console-consumer:

_--key-deserializer "org.apache.kafka.common.serialization.IntegerDeserializer"_

[source,shell script]
----
./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic refdata_geoLocation --property print.key=true --property key.separator=" - " --key-deserializer "org.apache.kafka.common.serialization.IntegerDeserializer"
0 - {"loc_id":0,"city":"Porrentruy","lon":47.415327,"lat":7.075221}
1 - {"loc_id":1,"city":"Geneva","lon":46.195602,"lat":6.148113}
2 - {"loc_id":2,"city":"Zürich","lon":47.366667,"lat":8.55}
3 - {"loc_id":3,"city":"Basel","lon":47.558395,"lat":7.573271}
4 - {"loc_id":4,"city":"Bern","lon":46.916667,"lat":7.466667}
5 - {"loc_id":5,"city":"Lausanne","lon":46.533333,"lat":6.666667}
6 - {"loc_id":6,"city":"Lucerne","lon":47.083333,"lat":8.266667}
...
----

=== starting producers: `iot`

----
java -classpath StreamAnalytics-0.2.0.0.jar producer.IoTSensorSimulatorAnomaly localhost:9092
----

=== Console output

[source,shell script]
----
./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic iot --property print.key=true --property key.separator=" - "

1636190810020:26 - {"sensor_ts":1636190810020,"sensor_id":26,"sensor_0":5,"sensor_1":9,"sensor_2":7,"sensor_3":29,"sensor_4":2,"sensor_5":39,"sensor_6":4,"sensor_7":46,"sensor_8":84,"sensor_9":45,"sensor_10":640,"sensor_11":1090}
1636190810225:30 - {"sensor_ts":1636190810225,"sensor_id":30,"sensor_0":5,"sensor_1":6,"sensor_2":18,"sensor_3":24,"sensor_4":39,"sensor_5":47,"sensor_6":35,"sensor_7":37,"sensor_8":42,"sensor_9":40,"sensor_10":842,"sensor_11":482}
1636190810432:6 - {"sensor_ts":1636190810432,"sensor_id":6,"sensor_0":2,"sensor_1":6,"sensor_2":14,"sensor_3":24,"sensor_4":41,"sensor_5":36,"sensor_6":42,"sensor_7":0,"sensor_8":27,"sensor_9":82,"sensor_10":254,"sensor_11":179}
1636190810637:37 - {"sensor_ts":1636190810637,"sensor_id":37,"sensor_0":1,"sensor_1":4,"sensor_2":1,"sensor_3":5,"sensor_4":2,"sensor_5":44,"sensor_6":40,"sensor_7":26,"sensor_8":42,"sensor_9":94,"sensor_10":357,"sensor_11":477}
1636190810842:36 - {"sensor_ts":1636190810842,"sensor_id":36,"sensor_0":3,"sensor_1":0,"sensor_2":10,"sensor_3":30,"sensor_4":23,"sensor_5":5,"sensor_6":54,"sensor_7":9,"sensor_8":21,"sensor_9":2,"sensor_10":569,"sensor_11":1016}
1636190811052:35 - {"sensor_ts":1636190811052,"sensor_id":35,"sensor_0":6,"sensor_1":9,"sensor_2":7,"sensor_3":14,"sensor_4":33,"sensor_5":23,"sensor_6":5,"sensor_7":58,"sensor_8":87,"sensor_9":50,"sensor_10":365,"sensor_11":742}

...
----
==  Create new Kafka topic

Get all running containers with `docker ps`

[source,shell]
----
$ docker ps
CONTAINER ID   IMAGE                                                                 COMMAND                  CREATED          STATUS                    PORTS                                              NAMES
be359da4753e   docker.repository.cloudera.com/csa/ssb-docker_flink:1.6.0.0-ce        "/usr/bin/init-flink…"   57 minutes ago   Up 57 minutes                                                                ssb-flink-taskmanager-1
b6561fb239c3   docker.repository.cloudera.com/csa/ssb-docker_flink:1.6.0.0-ce        "/usr/bin/init-flink…"   57 minutes ago   Up 57 minutes                                                                ssb-flink-taskmanager-2
50d266fd87af   docker.repository.cloudera.com/csa/ssb-docker_console:1.6.0.0-ce      "/opt/cloudera/entry…"   57 minutes ago   Up 56 minutes             0.0.0.0:8000->8000/tcp, 0.0.0.0:18111->18111/tcp   ssb-console-1
35b0e660b2de   docker.repository.cloudera.com/csa/ssb-docker_kafka:1.6.0.0-ce        "/usr/bin/init-kafka"    57 minutes ago   Up 56 minutes (healthy)   0.0.0.0:9092->9092/tcp                             ssb-kafka-1
cbd7e5c81c8d   docker.repository.cloudera.com/csa/ssb-docker_sqlio:1.6.0.0-ce        "/opt/cloudera/sqlio…"   57 minutes ago   Up 57 minutes (healthy)   0.0.0.0:18121->18121/tcp                           ssb-sqlio-1
33c64fccfcef   docker.repository.cloudera.com/csa/ssb-docker_zookeeper:1.6.0.0-ce    "/usr/bin/init-zooke…"   57 minutes ago   Up 57 minutes (healthy)   0.0.0.0:2181->2181/tcp                             ssb-zookeeper-1
b61c270719e2   docker.repository.cloudera.com/csa/ssb-docker_postgresql:1.6.0.0-ce   "pg_ctlcluster 12 ma…"   57 minutes ago   Up 57 minutes             0.0.0.0:5432->5432/tcp                             ssb-postgresql-1
69ca1ba7bde9   docker.repository.cloudera.com/csa/ssb-docker_snapper:1.6.0.0-ce      "/opt/cloudera/snapp…"   57 minutes ago   Up 57 minutes             8081/tcp, 0.0.0.0:18131->18131/tcp                 ssb-snapper-1
e40b8e0215fd   docker.repository.cloudera.com/csa/ssb-docker_flink:1.6.0.0-ce        "/usr/bin/init-flink…"   57 minutes ago   Up 57 minutes (healthy)   0.0.0.0:8081->8081/tcp                             ssb-flink-jobmanager-1
----
As you can see the Kafka broker is running in container = `35b0e660b2de`
Login into the Kafka container
[source,shell ]
----
$ docker exec -it 35b0e660b2de /bin/bash
----

After login:

. change into the kafka directory
. get a list of all kafka topics
. create a new kafka topics `sensor6_stats` with replication-factor 1 --partitions 1
. check again the list of kafka topics


[source,shell ]
----
kafka@35b0e660b2de:~$ cd /opt/kafka

kafka@35b0e660b2de:/opt/kafka$ ./bin/kafka-topics.sh --list --bootstrap-server localhost:9092
__consumer_offsets
__smm_producer_metrics
iot
refdata_geoLocation

kafka@35b0e660b2de:/opt/kafka$ ./bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic sensor6_stats
Created topic sensor6_stats.

kafka@35b0e660b2de:/opt/kafka$ ./bin/kafka-topics.sh --list --bootstrap-server localhost:9092
__consumer_offsets
__smm_producer_metrics
iot
refdata_geoLocation
sensor6_stats

kafka@35b0e660b2de:/opt/kafka

----

==  Analytics Event Streaming

Once everything is up and running, you can reach the SQL Stream Builder Console at: http://localhost:8000[localhost:8000] +
The default login and password are “admin" / "admin”.

Quick intro in how to use the Streaming SQL Console: https://docs.cloudera.com/csa/1.5.1/ssb-sql-console/topics/csa-ssb-using-console.html[official SSB DOC]

=== Create a table

[source,sql]
----
SHOW Tables;
DROP TABLE `RefData_GeoLocation` IF EXISTS;

CREATE TABLE `RefData_GeoLocation` (
  `loc_id` INT,
  `city` STRING,
  `lon` DOUBLE,
  `lat` DOUBLE,
  `eventTimestamp` TIMESTAMP(3) METADATA FROM 'timestamp',
  WATERMARK FOR `eventTimestamp` AS `eventTimestamp` - INTERVAL '15' SECONDS,
  PRIMARY KEY (loc_id) NOT ENFORCED
) COMMENT 'RefData_GeoLocation'
WITH (
  'connector' = 'upsert-kafka',
  'topic' = 'refdata_geoLocation',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'RefData_GeoLocation',
  'key.format' = 'raw',
  'value.format' = 'json'
);


DROP TABLE `IoT_Raw` IF EXISTS;

CREATE TABLE `IoT_Raw` (
  `sensor_ts` BIGINT,
  `sensor_id` INT,
  `sensor_0` BIGINT,
  `sensor_1` BIGINT,
  `sensor_2` BIGINT,
  `sensor_3` BIGINT,
  `sensor_4` BIGINT,
  `sensor_5` BIGINT,
  `sensor_6` BIGINT,
  `sensor_7` BIGINT,
  `sensor_8` BIGINT,
  `sensor_9` BIGINT,
  `sensor_10` BIGINT,
  `sensor_11` BIGINT,
  `eventTimestamp` TIMESTAMP(3) METADATA FROM 'timestamp',
  WATERMARK FOR `eventTimestamp` AS `eventTimestamp` - INTERVAL '3' SECOND
) COMMENT 'iot_enriched_source'
WITH (
  'connector' = 'kafka',
  'topic' = 'iot',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.auto.offset.reset' = 'earliest',
  'format' = 'json',
  'scan.startup.mode' = 'earliest-offset',
  'properties.group.id' = 'iot'
);
----

=== Create a query
This query will compute aggregates over 30-seconds windows that slide forward every second. For a specific sensor value in the record (`sensor_6`) it computes the following aggregations for each window:

* Number of events received
* Sum of the `sensor_6` value for all the events
* Average of the `sensor_6` value across all the events
* Min and max values of the `sensor_6` field
* Number of events for which the `sensor_6` value exceeds `70`

On the SSB UI:

. Click on Console (on the left bar) *> Compose > SQL*
+
. Enter `Sensor6Stats` for the *SQL Job Name* field.
+
. In the SQL box type the query shown below.
+
[source,sql]
----
INSERT INTO sensor6_stats_sink
SELECT
  i.`sensor_id` as device_id,
  geo.`city`,
  geo.`lon`,
  geo.`lat`,
  HOP_END(i.`eventTimestamp`, INTERVAL '1' SECOND, INTERVAL '30' SECOND) as windowEnd,
  count(*) as sensorCount,
  sum(`sensor_6`) as sensorSum,
  avg(cast(`sensor_6` as float)) as sensorAverage,
  min(`sensor_6`) as sensorMin,
  max(`sensor_6`) as sensorMax,
  sum(case when `sensor_6` > 70 then 1 else 0 end) as sensorGreaterThan60
FROM `IoT_Raw` i
JOIN `RefData_GeoLocation` FOR SYSTEM_TIME AS OF i.`eventTimestamp` AS geo
ON i.`sensor_id` = geo.`loc_id`
GROUP BY
  i.`sensor_id`, geo.`city`, geo.`lon`, geo.`lat`,
  HOP(i.`eventTimestamp`, INTERVAL '1' SECOND, INTERVAL '30' SECOND);
----

. Before you can execute this query, though, the `sensor6_stats_sink` table must be created in SSB, mapping it to the `sensor6_stats` Kafka topic.

+
. Since we want the topic format to be JSON, click on *Templates > cdp-local > JSON*.
+
image::../../images/template-kafka-json.png[width=300]

. This will prepend a `CREATE TABLE` DDL to the SQL script to create a table that matches the structure of your query!
+
image::../../images/template-table-ddl.png[width=800]
+
. Most of the table properties are already filled in for you. But there's one you must edit before you execute the statement: the `topic` property.
+
Edit the DDL statement and replace the `...` value of the `topic` property with the actual topic name: `sensor6_stats`.
+
image::../../images/template-table-edited.png[width=300]
+
. Click *Execute*.
+
. Scroll to the bottom of the page and you will see the log messages generated by your query execution.
+
. Let’s query the `sensor6_stats` topic to examine the data that is being written to it. Create a new job via `+ New Job`
+
NOTE: The `sensor6_stats` job will continue to run in the background. You can monitor and manage it through the *SQL Jobs* page.

+
[source,sql]
----
CREATE TABLE `sensor6_stats_source` (
    `device_id` BIGINT,
    `device_id` BIGINT,
    `city` STRING,
    `lon` DOUBLE,
    `lat` DOUBLE,
    `windowEnd` STRING,
    `sensorCount` BIGINT,
    `sensorSum` BIGINT,
    `sensorAverage` DOUBLE,
    `sensorMin` BIGINT,
    `sensorMax` BIGINT,
    `sensorGreaterThan60` BIGINT,
    `eventTimestamp` TIMESTAMP(3) METADATA FROM 'timestamp',  WATERMARK FOR `eventTimestamp` AS `eventTimestamp` - INTERVAL '3' SECOND)
WITH (
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.auto.offset.reset' = 'earliest',
  'connector' = 'kafka',
  'format' = 'json',
  'topic' = 'sensor6_stats',
  'scan.startup.mode' = 'earliest-offset',
  'properties.group.id' = 'sensor6_stats_source-consumer-1'
);
----
+
Enter the following query in the SQL field and execute it:
+
[source,sql]
----
SELECT * FROM sensor6_stats_source ;
----
+
[WARNING]
====
Make sure to stop your queries to release all resources once you finish. CSA CE is limited to a few worker tasks. You can double-check that all queries/jobs have been stopped by clicking on the SQL Jobs tab. If any jobs are still running, you can stop them from that page.
====